{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c025ddd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV salvo em: C:\\Users\\caval\\Projetos\\COLIN_WS\\csv_corrigidos\\telemetry_peak_daily_speed.csv\n",
      "✅ Tabela recriada no banco: Fazenda_WS.telemetry_peak_daily_speed\n",
      "✅ CSV salvo em: C:\\Users\\caval\\Projetos\\COLIN_WS\\csv_corrigidos\\telemetry_location.csv\n",
      "✅ Dados atualizados no banco! Total: 2626 registros.\n",
      "✅ CSV salvo em: C:\\Users\\caval\\Projetos\\COLIN_WS\\csv_corrigidos\\telemetry_fuel_used_last.csv\n",
      "✅ Tabela recriada no banco: Fazenda_WS.telemetry_fuel_used_last\n",
      "✅ CSV salvo em: C:\\Users\\caval\\Projetos\\COLIN_WS\\csv_corrigidos\\telemetry_fuel_remaining_ratio.csv\n",
      "✅ Tabela atualizada no banco: Fazenda_WS.telemetry_fuel_remaining_ratio\n",
      "✅ CSV salvo em: C:\\Users\\caval\\Projetos\\COLIN_WS\\csv_corrigidos\\telemetry_faults.csv\n",
      "🧹 Tabela telemetry_faults limpa com DELETE\n",
      "✅ Dados atualizados no banco: Fazenda_WS.telemetry_faults (Total: 1697 registros)\n",
      "✅ CSV salvo em: C:\\Users\\caval\\Projetos\\COLIN_WS\\csv_corrigidos\\telemetry_engine_condition.csv\n",
      "✅ Tabela recriada no banco: Fazenda_WS.telemetry_engine_condition\n",
      "✅ CSV salvo em: C:\\Users\\caval\\Projetos\\COLIN_WS\\csv_corrigidos\\telemetry_distance.csv\n",
      "✅ Tabela recriada no banco: Fazenda_WS.telemetry_distance\n",
      "✅ CSV salvo em: C:\\Users\\caval\\Projetos\\COLIN_WS\\csv_corrigidos\\telemetry_def_remaining.csv\n",
      "✅ Tabela recriada no banco: Fazenda_WS.telemetry_def_remaining\n",
      "✅ CSV salvo em: C:\\Users\\caval\\Projetos\\COLIN_WS\\csv_corrigidos\\telemetry_cumul_op_hours.csv\n",
      "✅ Tabela recriada no banco: Fazenda_WS.telemetry_cumul_op_hours\n",
      "✅ CSV salvo em: C:\\Users\\caval\\Projetos\\COLIN_WS\\csv_corrigidos\\telemetry_cumul_non_prod_idle_hours.csv\n",
      "✅ Tabela recriada no banco: Fazenda_WS.telemetry_cumul_non_prod_idle_hours\n",
      "✅ CSV salvo em: C:\\Users\\caval\\Projetos\\COLIN_WS\\csv_corrigidos\\telemetry_cumul_idle_hours.csv\n",
      "✅ Tabela recriada no banco: Fazenda_WS.telemetry_cumul_idle_hours\n",
      "✅ CSV salvo em: C:\\Users\\caval\\Projetos\\COLIN_WS\\csv_corrigidos\\telemetry_cumul_fuel_used.csv\n",
      "✅ Tabela recriada no banco: Fazenda_WS.telemetry_cumul_fuel_used\n",
      "✅ CSV salvo em: C:\\Users\\caval\\Projetos\\COLIN_WS\\csv_corrigidos\\telemetry_caution_codes.csv\n",
      "🧹 Tabela telemetry_caution_codes limpa com DELETE\n",
      "✅ Dados atualizados no banco: Fazenda_WS.telemetry_caution_codes (Total: 5745 registros)\n",
      "✅ CSV salvo em: C:\\Users\\caval\\Projetos\\COLIN_WS\\csv_corrigidos\\telemetry_average_load.csv\n",
      "✅ Tabela recriada no banco: Fazenda_WS.telemetry_average_load\n",
      "✅ CSV salvo em: C:\\Users\\caval\\Projetos\\COLIN_WS\\csv_corrigidos\\operations_by_vehicle.csv\n",
      "✅ Tabela recriada no banco: Fazenda_WS.operations_by_vehicle\n",
      "✅ CSV salvo em: C:\\Users\\caval\\Projetos\\COLIN_WS\\csv_corrigidos\\dim_equipment.csv\n",
      "✅ Tabela dim_equipment atualizada no schema Fazenda_WS\n",
      "✅ CSV salvo em: C:\\Users\\caval\\Projetos\\COLIN_WS\\csv_corrigidos\\dim_farm_setup_company_details.csv\n",
      "✅ Tabela dim_farm_setup_company_details atualizada no schema Fazenda_WS\n",
      "✅ CSV salvo em: C:\\Users\\caval\\Projetos\\COLIN_WS\\csv_corrigidos\\dim_farm_setup_fields.csv\n",
      "✅ Tabela dim_farm_setup_fields atualizada no schema Fazenda_WS\n",
      "✅ CSV salvo em: C:\\Users\\caval\\Projetos\\COLIN_WS\\csv_corrigidos\\dim_farm_setup_growers.csv\n",
      "✅ Tabela dim_farm_setup_growers atualizada no schema Fazenda_WS\n",
      "✅ CSV salvo em: C:\\Users\\caval\\Projetos\\COLIN_WS\\csv_corrigidos\\dim_farm_setup_user_profile.csv\n",
      "✅ Tabela dim_farm_setup_user_profile atualizada no schema Fazenda_WS\n",
      "✅ CSV salvo em: C:\\Users\\caval\\Projetos\\COLIN_WS\\csv_corrigidos\\dim_files.csv\n",
      "✅ Tabela dim_files atualizada no schema Fazenda_WS\n",
      "✅ CSV salvo em: C:\\Users\\caval\\Projetos\\COLIN_WS\\csv_corrigidos\\dim_rx_direct_to_vehicle_activity_types.csv\n",
      "✅ Tabela dim_rx_direct_to_vehicle_activity_types criada/recriada no schema Fazenda_WS\n",
      "✅ CSV salvo em: C:\\Users\\caval\\Projetos\\COLIN_WS\\csv_corrigidos\\dim_rx_direct_to_vehicle_chemical_categories.csv\n",
      "✅ Tabela dim_rx_direct_to_vehicle_chemical_categories criada/recriada no schema Fazenda_WS\n",
      "✅ CSV salvo em: C:\\Users\\caval\\Projetos\\COLIN_WS\\csv_corrigidos\\dim_rx_direct_to_vehicle_chemicals.csv\n",
      "✅ Tabela dim_rx_direct_to_vehicle_chemicals criada/recriada no schema Fazenda_WS\n",
      "✅ CSV salvo em: C:\\Users\\caval\\Projetos\\COLIN_WS\\csv_corrigidos\\dim_rx_direct_to_vehicle_crops.csv\n",
      "✅ Tabela dim_rx_direct_to_vehicle_crops criada/recriada no schema Fazenda_WS\n",
      "✅ CSV salvo em: C:\\Users\\caval\\Projetos\\COLIN_WS\\csv_corrigidos\\dim_rx_direct_to_vehicle_element_types.csv\n",
      "✅ Tabela dim_rx_direct_to_vehicle_element_types criada/recriada no schema Fazenda_WS\n",
      "✅ CSV salvo em: C:\\Users\\caval\\Projetos\\COLIN_WS\\csv_corrigidos\\dim_rx_direct_to_vehicle_genetic_types.csv\n",
      "✅ Tabela dim_rx_direct_to_vehicle_genetic_types criada/recriada no schema Fazenda_WS\n",
      "✅ CSV salvo em: C:\\Users\\caval\\Projetos\\COLIN_WS\\csv_corrigidos\\dim_rx_direct_to_vehicle_product_forms.csv\n",
      "✅ Tabela dim_rx_direct_to_vehicle_product_forms criada/recriada no schema Fazenda_WS\n",
      "✅ CSV salvo em: C:\\Users\\caval\\Projetos\\COLIN_WS\\csv_corrigidos\\dim_rx_direct_to_vehicle_seasons.csv\n",
      "✅ Tabela dim_rx_direct_to_vehicle_seasons criada/recriada no schema Fazenda_WS\n",
      "✅ CSV salvo em: C:\\Users\\caval\\Projetos\\COLIN_WS\\csv_corrigidos\\dim_rx_direct_to_vehicle_seed_brands.csv\n",
      "✅ Tabela dim_rx_direct_to_vehicle_seed_brands criada/recriada no schema Fazenda_WS\n",
      "✅ CSV salvo em: C:\\Users\\caval\\Projetos\\COLIN_WS\\csv_corrigidos\\dim_rx_direct_to_vehicle_seed_manufacturers.csv\n",
      "✅ Tabela dim_rx_direct_to_vehicle_seed_manufacturers criada/recriada no schema Fazenda_WS\n",
      "✅ CSV salvo em: C:\\Users\\caval\\Projetos\\COLIN_WS\\csv_corrigidos\\dim_rx_direct_to_vehicle_seeds.csv\n",
      "✅ Tabela dim_rx_direct_to_vehicle_seeds criada/recriada no schema Fazenda_WS\n",
      "✅ CSV salvo em: C:\\Users\\caval\\Projetos\\COLIN_WS\\csv_corrigidos\\dim_rx_direct_to_vehicle_units_of_measure.csv\n",
      "✅ Tabela dim_rx_direct_to_vehicle_units_of_measure criada/recriada no schema Fazenda_WS\n",
      "✅ CSV salvo em: C:\\Users\\caval\\Projetos\\COLIN_WS\\csv_corrigidos\\dim_rx_direct_to_vehicle_units_of_measure_types.csv\n",
      "✅ Tabela dim_rx_direct_to_vehicle_units_of_measure_types criada/recriada no schema Fazenda_WS\n",
      "📡 Obtendo dados atualizados da API...\n",
      "✅ Atualização concluída! 5 datas atualizadas, 0 novas datas adicionadas.\n",
      "Dados importados com sucesso para o schema Fazenda_WS.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import sqlalchemy \n",
    "from sqlalchemy import create_engine\n",
    "import matplotlib.pyplot as plt\n",
    "import unicodedata\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "# Configurações Gerais\n",
    "namenode = \"172.21.0.142\"\n",
    "port = 9870\n",
    "user = \"hdoop\"\n",
    "hdfs_path = \"/bronze/agrin/suprema/fat\"\n",
    "\n",
    "# Diretório de saída local\n",
    "output_dir = r\"C:\\Users\\caval\\Projetos\\COLIN_WS\\csv_corrigidos\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Conexão com PostgreSQL\n",
    "password = quote_plus(\"@Winover2024\")\n",
    "engine = create_engine(\n",
    "    f\"postgresql+psycopg2://wesley.carnauba:{password}@172.21.0.119/warin\",\n",
    "    connect_args={'client_encoding': 'utf8'}\n",
    ")\n",
    "\n",
    "# Funções Auxiliares\n",
    "def listar_arquivos(prefix):\n",
    "    url = f\"http://{namenode}:{port}/webhdfs/v1{hdfs_path}?op=LISTSTATUS&user.name={user}\"\n",
    "    r = requests.get(url)\n",
    "    r.raise_for_status()\n",
    "    files = r.json()[\"FileStatuses\"][\"FileStatus\"]\n",
    "    return [f[\"pathSuffix\"] for f in files if f[\"pathSuffix\"].startswith(prefix)]\n",
    "\n",
    "def abrir_json(path_suffix):\n",
    "    url = f\"http://{namenode}:{port}/webhdfs/v1{hdfs_path}/{path_suffix}?op=OPEN&user.name={user}\"\n",
    "    r1 = requests.get(url, allow_redirects=False)\n",
    "    redirect = r1.headers.get(\"Location\")\n",
    "    if not redirect:\n",
    "        raise Exception(f\"Sem redirecionamento para: {path_suffix}\")\n",
    "    redirect = redirect.replace(\"trisk05\", namenode).replace(\"trisk06\", namenode).replace(\"trisk10\", namenode)\n",
    "    r2 = requests.get(redirect)\n",
    "    r2.raise_for_status()\n",
    "    return r2.json()\n",
    "\n",
    "def extrair_equipamento(link):\n",
    "    match = re.search(r\"/Equipment/([^/]+)/\", link)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def extrair_valor_speed(val):\n",
    "    if isinstance(val, str):\n",
    "        try:\n",
    "            val = json.loads(val)\n",
    "        except:\n",
    "            return 0\n",
    "    if not val:\n",
    "        return 0\n",
    "    if isinstance(val, list) and isinstance(val[0], dict) and \"Speed\" in val[0]:\n",
    "        return val[0][\"Speed\"]\n",
    "    return 0\n",
    "\n",
    "\n",
    "# ### Parte 2 - Telemetry Peak Daily Speed\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "prefix = \"telemetry_peak_daily_speed_\"\n",
    "output_csv = os.path.join(output_dir, \"telemetry_peak_daily_speed.csv\")\n",
    "tabela_banco = \"telemetry_peak_daily_speed\"\n",
    "\n",
    "# Coleta os arquivos do HDFS\n",
    "arquivos = listar_arquivos(prefix)\n",
    "todos_dados = []\n",
    "\n",
    "for arq in arquivos:\n",
    "    try:\n",
    "        dados = abrir_json(arq)\n",
    "        for item in dados:\n",
    "            links = item.get(\"Links\", [])\n",
    "            hrefs = [l[\"href\"] for l in links if l[\"rel\"] == \"self\"]\n",
    "            if hrefs:\n",
    "                equipamento = extrair_equipamento(hrefs[0])\n",
    "                todos_dados.append({\n",
    "                    \"Arquivo\": arq,\n",
    "                    \"Equipamento\": equipamento,\n",
    "                    \"Speed\": item.get(\"Speed\", [])\n",
    "                })\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Erro ao processar {arq}: {e}\")\n",
    "\n",
    "# Cria DataFrame e trata colunas\n",
    "df = pd.DataFrame(todos_dados)\n",
    "df[\"Data\"] = df[\"Arquivo\"].str.extract(r\"_(\\d{4}-\\d{2}-\\d{2})_\")\n",
    "df[\"Speed\"] = df[\"Speed\"].apply(extrair_valor_speed)\n",
    "df = df[[\"Equipamento\", \"Speed\", \"Data\"]]\n",
    "\n",
    "# Salva CSV\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"✅ CSV salvo em: {output_csv}\")\n",
    "\n",
    "# Insere no banco, recriando a tabela\n",
    "df.to_sql(tabela_banco, engine, schema=\"Fazenda_WS\", if_exists=\"replace\", index=False)\n",
    "print(f\"✅ Tabela recriada no banco: Fazenda_WS.{tabela_banco}\")\n",
    "\n",
    "\n",
    "# ### Parte 3 - Telemetry Location\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import text\n",
    "\n",
    "prefix = \"telemetry_location_\"\n",
    "output_csv = os.path.join(output_dir, \"telemetry_location.csv\")\n",
    "output_excel = os.path.join(output_dir, \"telemetry_location.xlsx\")\n",
    "tabela_banco = \"telemetry_location\"\n",
    "\n",
    "arquivos = listar_arquivos(prefix)\n",
    "todos_dados = []\n",
    "\n",
    "for arq in arquivos:\n",
    "    try:\n",
    "        dados = abrir_json(arq)\n",
    "        for item in dados:\n",
    "            links = item.get(\"Links\", [])\n",
    "            hrefs = [l[\"href\"] for l in links if l[\"rel\"] == \"self\"]\n",
    "            equipamento = extrair_equipamento(hrefs[0]) if hrefs else None\n",
    "            localizacoes = item.get(\"Location\", [])\n",
    "            for loc in localizacoes:\n",
    "                todos_dados.append({\n",
    "                    \"Equipamento\": equipamento,\n",
    "                    \"DataHora\": loc.get(\"@datetime\"),\n",
    "                    \"Latitude\": loc.get(\"Latitude\"),\n",
    "                    \"Longitude\": loc.get(\"Longitude\"),\n",
    "                    \"Altitude\": loc.get(\"Altitude\"),\n",
    "                    \"UnidadeAltitude\": loc.get(\"AltitudeUnits\"),\n",
    "                    \"Arquivo\": arq\n",
    "                })\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Erro ao processar {arq}: {e}\")\n",
    "\n",
    "# Criação do DataFrame\n",
    "df = pd.DataFrame(todos_dados)\n",
    "\n",
    "# Extrai a data do nome do arquivo\n",
    "df[\"Data\"] = df[\"Arquivo\"].str.extract(r\"_(\\d{4}-\\d{2}-\\d{2})_\")\n",
    "\n",
    "# Organiza colunas e remove \"Arquivo\"\n",
    "df = df[[\"Equipamento\", \"DataHora\", \"Latitude\", \"Longitude\", \"Altitude\", \"UnidadeAltitude\", \"Data\"]]\n",
    "\n",
    "# Salva CSV\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"✅ CSV salvo em: {output_csv}\")\n",
    "\n",
    "# ============== ÚNICA ALTERAÇÃO FEITA ==============\n",
    "# Substituição do DROP/REPLACE por TRUNCATE/APPEND\n",
    "with engine.begin() as conn:\n",
    "    # 1. Limpa a tabela sem dropar\n",
    "    conn.execute(text(f'DELETE FROM \"Fazenda_WS\".\"{tabela_banco}\"'))\n",
    "\n",
    "    df.to_sql(\n",
    "        tabela_banco,\n",
    "        engine,\n",
    "        schema=\"Fazenda_WS\",\n",
    "        if_exists=\"append\",\n",
    "        index=False,\n",
    "        method=\"multi\"\n",
    "    )\n",
    "\n",
    "print(f\"✅ Dados atualizados no banco! Total: {len(df)} registros.\")\n",
    "\n",
    "\n",
    "# ### Parte 4 - Telemetry Fuel Used Last 24h\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "prefix = \"telemetry_fuel_used_last_\"\n",
    "output_csv = os.path.join(output_dir, \"telemetry_fuel_used_last.csv\")\n",
    "output_excel = os.path.join(output_dir, \"telemetry_fuel_used_last.xlsx\")\n",
    "tabela_banco = \"telemetry_fuel_used_last\"\n",
    "\n",
    "arquivos = listar_arquivos(prefix)\n",
    "todos_dados = []\n",
    "\n",
    "for arq in arquivos:\n",
    "    try:\n",
    "        dados = abrir_json(arq)\n",
    "        for item in dados:\n",
    "            links = item.get(\"Links\", [])\n",
    "            hrefs = [l[\"href\"] for l in links if l[\"rel\"] == \"self\"]\n",
    "            equipamento = extrair_equipamento(hrefs[0]) if hrefs else None\n",
    "\n",
    "            registros = item.get(\"FuelUsedLast24\", [])\n",
    "            if not registros:\n",
    "                todos_dados.append({\n",
    "                    \"Equipamento\": equipamento,\n",
    "                    \"FuelUsed\": 0,\n",
    "                    \"DataHora\": None,\n",
    "                    \"Unidade\": None,\n",
    "                    \"Arquivo\": arq\n",
    "                })\n",
    "            else:\n",
    "                for dado in registros:\n",
    "                    todos_dados.append({\n",
    "                        \"Equipamento\": equipamento,\n",
    "                        \"FuelUsed\": dado.get(\"FuelUsed\", 0),\n",
    "                        \"DataHora\": dado.get(\"@datetime\"),\n",
    "                        \"Unidade\": dado.get(\"FuelUnits\"),\n",
    "                        \"Arquivo\": arq\n",
    "                    })\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Erro ao processar {arq}: {e}\")\n",
    "\n",
    "# Criação do DataFrame\n",
    "df = pd.DataFrame(todos_dados)\n",
    "\n",
    "# Extrai a data do nome do arquivo\n",
    "df[\"Data\"] = df[\"Arquivo\"].str.extract(r\"_(\\d{4}-\\d{2}-\\d{2})_\")\n",
    "\n",
    "# Organiza colunas e remove \"Arquivo\"\n",
    "df = df[[\"Equipamento\", \"FuelUsed\", \"Unidade\", \"DataHora\", \"Data\"]]\n",
    "\n",
    "# Salva CSV\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"✅ CSV salvo em: {output_csv}\")\n",
    "\n",
    "# Insere no banco\n",
    "df.to_sql(tabela_banco, engine, schema=\"Fazenda_WS\", if_exists=\"replace\", index=False)\n",
    "print(f\"✅ Tabela recriada no banco: Fazenda_WS.{tabela_banco}\")\n",
    "\n",
    "\n",
    "# ### Parte 5 - Telemetry Fuel Remaining Ratio\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "\n",
    "prefix = \"telemetry_fuel_remaining_ratio_\"\n",
    "output_csv = os.path.join(output_dir, \"telemetry_fuel_remaining_ratio.csv\")\n",
    "output_excel = os.path.join(output_dir, \"telemetry_fuel_remaining_ratio.xlsx\")\n",
    "tabela_banco = \"telemetry_fuel_remaining_ratio\"\n",
    "\n",
    "arquivos = listar_arquivos(prefix)\n",
    "todos_dados = []\n",
    "\n",
    "for arq in arquivos:\n",
    "    try:\n",
    "        dados = abrir_json(arq)\n",
    "        for item in dados:\n",
    "            links = item.get(\"Links\", [])\n",
    "            hrefs = [l[\"href\"] for l in links if l[\"rel\"] == \"self\"]\n",
    "            equipamento = extrair_equipamento(hrefs[0]) if hrefs else None\n",
    "            niveis = item.get(\"FuelRemaining\", [])\n",
    "            for registro in niveis:\n",
    "                todos_dados.append({\n",
    "                    \"Equipamento\": equipamento,\n",
    "                    \"DataHora\": registro.get(\"@datetime\"),\n",
    "                    \"Percentual\": registro.get(\"percent\"),\n",
    "                    \"Arquivo\": arq\n",
    "                })\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Erro ao processar {arq}: {e}\")\n",
    "\n",
    "# Criação do DataFrame\n",
    "df = pd.DataFrame(todos_dados)\n",
    "\n",
    "# Extrai a data do nome do arquivo\n",
    "df[\"Data\"] = df[\"Arquivo\"].str.extract(r\"_(\\d{4}-\\d{2}-\\d{2})_\")\n",
    "\n",
    "# Organiza colunas\n",
    "df = df[[\"Equipamento\", \"Percentual\", \"DataHora\", \"Data\"]]\n",
    "\n",
    "# Salva CSV\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"✅ CSV salvo em: {output_csv}\")\n",
    "\n",
    "# Salva no banco\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(f'DELETE FROM \"Fazenda_WS\".\"{tabela_banco}\"'))\n",
    "df.to_sql(tabela_banco, engine, schema=\"Fazenda_WS\", if_exists=\"append\", index=False)\n",
    "print(f\"✅ Tabela atualizada no banco: Fazenda_WS.{tabela_banco}\")\n",
    "\n",
    "\n",
    "# ### Parte 6 - Telemetry Faults\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "\n",
    "prefix = \"telemetry_faults_\"\n",
    "output_csv = os.path.join(output_dir, \"telemetry_faults.csv\")\n",
    "tabela_banco = \"telemetry_faults\"\n",
    "\n",
    "arquivos = listar_arquivos(prefix)\n",
    "todos_dados = []\n",
    "\n",
    "for arq in arquivos:\n",
    "    try:\n",
    "        dados = abrir_json(arq)\n",
    "        for item in dados:\n",
    "            links = item.get(\"Links\", [])\n",
    "            hrefs = [l[\"href\"] for l in links if l[\"rel\"] == \"self\"]\n",
    "            equipamento = extrair_equipamento(hrefs[0]) if hrefs else None\n",
    "\n",
    "            codigos = item.get(\"FaultCode\", [])\n",
    "            if not codigos:\n",
    "                todos_dados.append({\n",
    "                    \"Equipamento\": equipamento,\n",
    "                    \"FaultCode\": None,\n",
    "                    \"DataHora\": None,\n",
    "                    \"Severidade\": None,\n",
    "                    \"Descricao\": None,\n",
    "                    \"Arquivo\": arq\n",
    "                })\n",
    "            else:\n",
    "                for f in codigos:\n",
    "                    todos_dados.append({\n",
    "                        \"Equipamento\": equipamento,\n",
    "                        \"FaultCode\": f.get(\"CodeIdentifier\"),\n",
    "                        \"DataHora\": f.get(\"@datetime\"),\n",
    "                        \"Severidade\": f.get(\"CodeSeverity\"),\n",
    "                        \"Descricao\": f.get(\"CodeDescription\"),\n",
    "                        \"Arquivo\": arq\n",
    "                    })\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Erro ao processar {arq}: {e}\")\n",
    "\n",
    "# Cria DataFrame\n",
    "df = pd.DataFrame(todos_dados)\n",
    "\n",
    "# Extrai a data do nome do arquivo\n",
    "df[\"Data\"] = df[\"Arquivo\"].str.extract(r\"_(\\d{4}-\\d{2}-\\d{2})_\")\n",
    "\n",
    "# Organiza colunas\n",
    "df = df[[\"Equipamento\", \"FaultCode\", \"Severidade\", \"Descricao\", \"DataHora\", \"Data\"]]\n",
    "\n",
    "# Salva CSV\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"✅ CSV salvo em: {output_csv}\")\n",
    "\n",
    "# Atualiza o banco: DELETE + INSERT\n",
    "from sqlalchemy import text  # Certifique-se de importar isso\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(f'DELETE FROM \"Fazenda_WS\".\"{tabela_banco}\"'))\n",
    "    print(f\"🧹 Tabela {tabela_banco} limpa com DELETE\")\n",
    "\n",
    "df.to_sql(\n",
    "    tabela_banco,\n",
    "    engine,\n",
    "    schema=\"Fazenda_WS\",\n",
    "    if_exists=\"append\",\n",
    "    index=False,\n",
    "    method=\"multi\"  # melhora a performance em inserts grandes\n",
    ")\n",
    "\n",
    "print(f\"✅ Dados atualizados no banco: Fazenda_WS.{tabela_banco} (Total: {len(df)} registros)\")\n",
    "\n",
    "\n",
    "# ### Parte 7 - Telemetry Engine Condition\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "\n",
    "prefix = \"telemetry_engine_condition_\"\n",
    "output_csv = os.path.join(output_dir, \"telemetry_engine_condition.csv\")\n",
    "output_excel = os.path.join(output_dir, \"telemetry_engine_condition.xlsx\")\n",
    "tabela_banco = \"telemetry_engine_condition\"\n",
    "\n",
    "arquivos = listar_arquivos(prefix)\n",
    "todos_dados = []\n",
    "\n",
    "for arq in arquivos:\n",
    "    try:\n",
    "        dados = abrir_json(arq)\n",
    "        for item in dados:\n",
    "            links = item.get(\"Links\", [])\n",
    "            hrefs = [l[\"href\"] for l in links if l[\"rel\"] == \"self\"]\n",
    "            equipamento = extrair_equipamento(hrefs[0]) if hrefs else None\n",
    "\n",
    "            status = item.get(\"EngineStatus\", [])\n",
    "            for e in status:\n",
    "                todos_dados.append({\n",
    "                    \"Equipamento\": equipamento,\n",
    "                    \"DataHora\": e.get(\"@datetime\"),\n",
    "                    \"EngineNumber\": e.get(\"EngineNumber\"),\n",
    "                    \"Running\": e.get(\"Running\"),\n",
    "                    \"Arquivo\": arq\n",
    "                })\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Erro ao processar {arq}: {e}\")\n",
    "\n",
    "# DataFrame\n",
    "df = pd.DataFrame(todos_dados)\n",
    "\n",
    "# Extrai a data do nome do arquivo\n",
    "df[\"Data\"] = df[\"Arquivo\"].str.extract(r\"_(\\d{4}-\\d{2}-\\d{2})_\")\n",
    "\n",
    "# Organiza colunas\n",
    "df = df[[\"Equipamento\", \"EngineNumber\", \"Running\", \"DataHora\", \"Data\"]]\n",
    "\n",
    "# Salva CSV\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"✅ CSV salvo em: {output_csv}\")\n",
    "\n",
    "# Salva no banco\n",
    "df.to_sql(tabela_banco, engine, schema=\"Fazenda_WS\", if_exists=\"replace\", index=False)\n",
    "print(f\"✅ Tabela recriada no banco: Fazenda_WS.{tabela_banco}\")\n",
    "\n",
    "\n",
    "# ### Parte 8 - Telemetry Distance\n",
    "\n",
    "# In[27]:\n",
    "\n",
    "\n",
    "prefix = \"telemetry_distance_\"\n",
    "output_csv = os.path.join(output_dir, \"telemetry_distance.csv\")\n",
    "output_excel = os.path.join(output_dir, \"telemetry_distance.xlsx\")\n",
    "tabela_banco = \"telemetry_distance\"\n",
    "\n",
    "arquivos = listar_arquivos(prefix)\n",
    "todos_dados = []\n",
    "\n",
    "for arq in arquivos:\n",
    "    try:\n",
    "        dados = abrir_json(arq)\n",
    "        for item in dados:\n",
    "            links = item.get(\"Links\", [])\n",
    "            hrefs = [l[\"href\"] for l in links if l[\"rel\"] == \"self\"]\n",
    "            equipamento = extrair_equipamento(hrefs[0]) if hrefs else None\n",
    "\n",
    "            distancias = item.get(\"Distance\", [])\n",
    "            if not distancias:\n",
    "                todos_dados.append({\n",
    "                    \"Equipamento\": equipamento,\n",
    "                    \"Distance\": 0,\n",
    "                    \"Unidade\": None,\n",
    "                    \"DataHora\": None,\n",
    "                    \"Arquivo\": arq\n",
    "                })\n",
    "            else:\n",
    "                for d in distancias:\n",
    "                    todos_dados.append({\n",
    "                        \"Equipamento\": equipamento,\n",
    "                        \"Distance\": d.get(\"Distance\", 0),\n",
    "                        \"Unidade\": d.get(\"DistanceUnits\"),\n",
    "                        \"DataHora\": d.get(\"@datetime\"),\n",
    "                        \"Arquivo\": arq\n",
    "                    })\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Erro ao processar {arq}: {e}\")\n",
    "\n",
    "# DataFrame\n",
    "df = pd.DataFrame(todos_dados)\n",
    "\n",
    "# Extrai a data do nome do arquivo\n",
    "df[\"Data\"] = df[\"Arquivo\"].str.extract(r\"_(\\d{4}-\\d{2}-\\d{2})_\")\n",
    "\n",
    "# Organiza colunas\n",
    "df = df[[\"Equipamento\", \"Distance\", \"Unidade\", \"DataHora\", \"Data\"]]\n",
    "\n",
    "# Salva CSV\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"✅ CSV salvo em: {output_csv}\")\n",
    "\n",
    "# Salva no banco\n",
    "df.to_sql(tabela_banco, engine, schema=\"Fazenda_WS\", if_exists=\"replace\", index=False)\n",
    "print(f\"✅ Tabela recriada no banco: Fazenda_WS.{tabela_banco}\")\n",
    "\n",
    "\n",
    "# ### Parte 9 - Telemetry DEF Remaining\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "\n",
    "prefix = \"telemetry_def_remaining_\"\n",
    "output_csv = os.path.join(output_dir, \"telemetry_def_remaining.csv\")\n",
    "output_excel = os.path.join(output_dir, \"telemetry_def_remaining.xlsx\")\n",
    "tabela_banco = \"telemetry_def_remaining\"\n",
    "\n",
    "arquivos = listar_arquivos(prefix)\n",
    "todos_dados = []\n",
    "\n",
    "for arq in arquivos:\n",
    "    try:\n",
    "        dados = abrir_json(arq)\n",
    "        for item in dados:\n",
    "            links = item.get(\"Links\", [])\n",
    "            hrefs = [l[\"href\"] for l in links if l[\"rel\"] == \"self\"]\n",
    "            equipamento = extrair_equipamento(hrefs[0]) if hrefs else None\n",
    "\n",
    "            def_data = item.get(\"DEFRemaining\", [])\n",
    "            if not def_data:\n",
    "                todos_dados.append({\n",
    "                    \"Equipamento\": equipamento,\n",
    "                    \"DEFRemaining\": 0,\n",
    "                    \"Unidade\": None,\n",
    "                    \"DataHora\": None,\n",
    "                    \"Arquivo\": arq\n",
    "                })\n",
    "            else:\n",
    "                for d in def_data:\n",
    "                    todos_dados.append({\n",
    "                        \"Equipamento\": equipamento,\n",
    "                        \"DEFRemaining\": d.get(\"DEFRemaining\", 0),\n",
    "                        \"Unidade\": d.get(\"DEFUnits\"),\n",
    "                        \"DataHora\": d.get(\"@datetime\"),\n",
    "                        \"Arquivo\": arq\n",
    "                    })\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Erro ao processar {arq}: {e}\")\n",
    "\n",
    "# Criação do DataFrame\n",
    "df = pd.DataFrame(todos_dados)\n",
    "\n",
    "# Extrai a data do nome do arquivo\n",
    "df[\"Data\"] = df[\"Arquivo\"].str.extract(r\"_(\\d{4}-\\d{2}-\\d{2})_\")\n",
    "\n",
    "# Organiza colunas\n",
    "df = df[[\"Equipamento\", \"DEFRemaining\", \"Unidade\", \"DataHora\", \"Data\"]]\n",
    "\n",
    "# Salva CSV\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"✅ CSV salvo em: {output_csv}\")\n",
    "\n",
    "# Salva no banco\n",
    "df.to_sql(tabela_banco, engine, schema=\"Fazenda_WS\", if_exists=\"replace\", index=False)\n",
    "print(f\"✅ Tabela recriada no banco: Fazenda_WS.{tabela_banco}\")\n",
    "\n",
    "\n",
    "# ### Parte 10 - Telemetry Cumulative Operating Hours\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "\n",
    "prefix = \"telemetry_cumul_op_hours_\"\n",
    "output_csv = os.path.join(output_dir, \"telemetry_cumul_op_hours.csv\")\n",
    "output_excel = os.path.join(output_dir, \"telemetry_cumul_op_hours.xlsx\")\n",
    "tabela_banco = \"telemetry_cumul_op_hours\"\n",
    "\n",
    "arquivos = listar_arquivos(prefix)\n",
    "todos_dados = []\n",
    "\n",
    "for arq in arquivos:\n",
    "    try:\n",
    "        dados = abrir_json(arq)\n",
    "        for item in dados:\n",
    "            links = item.get(\"Links\", [])\n",
    "            hrefs = [l[\"href\"] for l in links if l[\"rel\"] == \"self\"]\n",
    "            equipamento = extrair_equipamento(hrefs[0]) if hrefs else None\n",
    "\n",
    "            horas = item.get(\"CumulativeOperatingHours\", [])\n",
    "            if not horas:\n",
    "                todos_dados.append({\n",
    "                    \"Equipamento\": equipamento,\n",
    "                    \"Hora\": 0,\n",
    "                    \"DataHora\": None,\n",
    "                    \"Arquivo\": arq\n",
    "                })\n",
    "            else:\n",
    "                for h in horas:\n",
    "                    todos_dados.append({\n",
    "                        \"Equipamento\": equipamento,\n",
    "                        \"Hora\": h.get(\"Hour\", 0),\n",
    "                        \"DataHora\": h.get(\"@datetime\"),\n",
    "                        \"Arquivo\": arq\n",
    "                    })\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Erro ao processar {arq}: {e}\")\n",
    "\n",
    "# Criação do DataFrame\n",
    "df = pd.DataFrame(todos_dados)\n",
    "\n",
    "# Extrai a data do nome do arquivo\n",
    "df[\"Data\"] = df[\"Arquivo\"].str.extract(r\"_(\\d{4}-\\d{2}-\\d{2})_\")\n",
    "\n",
    "# Organiza colunas\n",
    "df = df[[\"Equipamento\", \"Hora\", \"DataHora\", \"Data\"]]\n",
    "\n",
    "# Salva CSV\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"✅ CSV salvo em: {output_csv}\")\n",
    "\n",
    "\n",
    "# Salva no banco\n",
    "df.to_sql(tabela_banco, engine, schema=\"Fazenda_WS\", if_exists=\"replace\", index=False)\n",
    "print(f\"✅ Tabela recriada no banco: Fazenda_WS.{tabela_banco}\")\n",
    "\n",
    "\n",
    "# ### Parte 11 - Telemetry Cumulative Non-Productive Idle Hours\n",
    "\n",
    "# In[30]:\n",
    "\n",
    "\n",
    "prefix = \"telemetry_cumul_non_prod_idle_hours_\"\n",
    "output_csv = os.path.join(output_dir, \"telemetry_cumul_non_prod_idle_hours.csv\")\n",
    "output_excel = os.path.join(output_dir, \"telemetry_cumul_non_prod_idle_hours.xlsx\")\n",
    "tabela_banco = \"telemetry_cumul_non_prod_idle_hours\"\n",
    "\n",
    "arquivos = listar_arquivos(prefix)\n",
    "todos_dados = []\n",
    "\n",
    "for arq in arquivos:\n",
    "    try:\n",
    "        dados = abrir_json(arq)\n",
    "        for item in dados:\n",
    "            links = item.get(\"Links\", [])\n",
    "            hrefs = [l[\"href\"] for l in links if l[\"rel\"] == \"self\"]\n",
    "            equipamento = extrair_equipamento(hrefs[0]) if hrefs else None\n",
    "\n",
    "            horas = item.get(\"CumulativeNonProductiveIdleHours\", [])\n",
    "            if not horas:\n",
    "                todos_dados.append({\n",
    "                    \"Equipamento\": equipamento,\n",
    "                    \"Hora\": 0,\n",
    "                    \"DataHora\": None,\n",
    "                    \"Arquivo\": arq\n",
    "                })\n",
    "            else:\n",
    "                for h in horas:\n",
    "                    todos_dados.append({\n",
    "                        \"Equipamento\": equipamento,\n",
    "                        \"Hora\": h.get(\"Hour\", 0),\n",
    "                        \"DataHora\": h.get(\"@datetime\"),\n",
    "                        \"Arquivo\": arq\n",
    "                    })\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Erro ao processar {arq}: {e}\")\n",
    "\n",
    "# DataFrame\n",
    "df = pd.DataFrame(todos_dados)\n",
    "\n",
    "# Extrai a data do nome do arquivo\n",
    "df[\"Data\"] = df[\"Arquivo\"].str.extract(r\"_(\\d{4}-\\d{2}-\\d{2})_\")\n",
    "\n",
    "# Organiza colunas\n",
    "df = df[[\"Equipamento\", \"Hora\", \"DataHora\", \"Data\"]]\n",
    "\n",
    "# Salva CSV\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"✅ CSV salvo em: {output_csv}\")\n",
    "\n",
    "# Salva no banco\n",
    "df.to_sql(tabela_banco, engine, schema=\"Fazenda_WS\", if_exists=\"replace\", index=False)\n",
    "print(f\"✅ Tabela recriada no banco: Fazenda_WS.{tabela_banco}\")\n",
    "\n",
    "\n",
    "# ### Parte 12 - Telemetry Cumulative Idle Hours\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "\n",
    "prefix = \"telemetry_cumul_idle_hours_\"\n",
    "output_csv = os.path.join(output_dir, \"telemetry_cumul_idle_hours.csv\")\n",
    "output_excel = os.path.join(output_dir, \"telemetry_cumul_idle_hours.xlsx\")\n",
    "tabela_banco = \"telemetry_cumul_idle_hours\"\n",
    "\n",
    "arquivos = listar_arquivos(prefix)\n",
    "todos_dados = []\n",
    "\n",
    "for arq in arquivos:\n",
    "    try:\n",
    "        dados = abrir_json(arq)\n",
    "        for item in dados:\n",
    "            links = item.get(\"Links\", [])\n",
    "            hrefs = [l[\"href\"] for l in links if l[\"rel\"] == \"self\"]\n",
    "            equipamento = extrair_equipamento(hrefs[0]) if hrefs else None\n",
    "\n",
    "            horas = item.get(\"CumulativeIdleHours\", [])\n",
    "            if not horas:\n",
    "                todos_dados.append({\n",
    "                    \"Equipamento\": equipamento,\n",
    "                    \"Hora\": 0,\n",
    "                    \"DataHora\": None,\n",
    "                    \"Arquivo\": arq\n",
    "                })\n",
    "            else:\n",
    "                for h in horas:\n",
    "                    todos_dados.append({\n",
    "                        \"Equipamento\": equipamento,\n",
    "                        \"Hora\": h.get(\"Hour\", 0),\n",
    "                        \"DataHora\": h.get(\"@datetime\"),\n",
    "                        \"Arquivo\": arq\n",
    "                    })\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Erro ao processar {arq}: {e}\")\n",
    "\n",
    "# Criação do DataFrame\n",
    "df = pd.DataFrame(todos_dados)\n",
    "\n",
    "# Extrai a data do nome do arquivo\n",
    "df[\"Data\"] = df[\"Arquivo\"].str.extract(r\"_(\\d{4}-\\d{2}-\\d{2})_\")\n",
    "\n",
    "# Organiza colunas\n",
    "df = df[[\"Equipamento\", \"Hora\", \"DataHora\", \"Data\"]]\n",
    "\n",
    "# Salva CSV\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"✅ CSV salvo em: {output_csv}\")\n",
    "\n",
    "# Salva no banco\n",
    "df.to_sql(tabela_banco, engine, schema=\"Fazenda_WS\", if_exists=\"replace\", index=False)\n",
    "print(f\"✅ Tabela recriada no banco: Fazenda_WS.{tabela_banco}\")\n",
    "\n",
    "\n",
    "# ### Parte 13 - Telemetry Cumulative Fuel Used\n",
    "\n",
    "# In[32]:\n",
    "\n",
    "\n",
    "prefix = \"telemetry_cumul_fuel_used_\"\n",
    "output_csv = os.path.join(output_dir, \"telemetry_cumul_fuel_used.csv\")\n",
    "output_excel = os.path.join(output_dir, \"telemetry_cumul_fuel_used.xlsx\")\n",
    "tabela_banco = \"telemetry_cumul_fuel_used\"\n",
    "\n",
    "arquivos = listar_arquivos(prefix)\n",
    "todos_dados = []\n",
    "\n",
    "for arq in arquivos:\n",
    "    try:\n",
    "        dados = abrir_json(arq)\n",
    "        for item in dados:\n",
    "            links = item.get(\"Links\", [])\n",
    "            hrefs = [l[\"href\"] for l in links if l[\"rel\"] == \"self\"]\n",
    "            equipamento = extrair_equipamento(hrefs[0]) if hrefs else None\n",
    "\n",
    "            combustivel = item.get(\"FuelUsed\", [])\n",
    "            if not combustivel:\n",
    "                todos_dados.append({\n",
    "                    \"Equipamento\": equipamento,\n",
    "                    \"FuelConsumed\": 0,\n",
    "                    \"Unidade\": None,\n",
    "                    \"DataHora\": None,\n",
    "                    \"Arquivo\": arq\n",
    "                })\n",
    "            else:\n",
    "                for f in combustivel:\n",
    "                    todos_dados.append({\n",
    "                        \"Equipamento\": equipamento,\n",
    "                        \"FuelConsumed\": f.get(\"FuelConsumed\", 0),\n",
    "                        \"Unidade\": f.get(\"FuelUnits\"),\n",
    "                        \"DataHora\": f.get(\"@datetime\"),\n",
    "                        \"Arquivo\": arq\n",
    "                    })\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Erro ao processar {arq}: {e}\")\n",
    "\n",
    "# Criação do DataFrame\n",
    "df = pd.DataFrame(todos_dados)\n",
    "\n",
    "# Extrai a data do nome do arquivo\n",
    "df[\"Data\"] = df[\"Arquivo\"].str.extract(r\"_(\\d{4}-\\d{2}-\\d{2})_\")\n",
    "\n",
    "# Organiza colunas\n",
    "df = df[[\"Equipamento\", \"FuelConsumed\", \"Unidade\", \"DataHora\", \"Data\"]]\n",
    "\n",
    "# Salva CSV\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"✅ CSV salvo em: {output_csv}\")\n",
    "\n",
    "# Salva no banco\n",
    "df.to_sql(tabela_banco, engine, schema=\"Fazenda_WS\", if_exists=\"replace\", index=False)\n",
    "print(f\"✅ Tabela recriada no banco: Fazenda_WS.{tabela_banco}\")\n",
    "\n",
    "\n",
    "# ### Parte 14 - Telemetry Caution Codes\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "\n",
    "prefix = \"telemetry_caution_codes_\"\n",
    "output_csv = os.path.join(output_dir, \"telemetry_caution_codes.csv\")\n",
    "output_excel = os.path.join(output_dir, \"telemetry_caution_codes.xlsx\")\n",
    "tabela_banco = \"telemetry_caution_codes\"\n",
    "\n",
    "arquivos = listar_arquivos(prefix)\n",
    "todos_dados = []\n",
    "\n",
    "for arq in arquivos:\n",
    "    try:\n",
    "        dados = abrir_json(arq)\n",
    "        for item in dados:\n",
    "            links = item.get(\"Links\", [])\n",
    "            hrefs = [l[\"href\"] for l in links if l[\"rel\"] == \"self\"]\n",
    "            equipamento = extrair_equipamento(hrefs[0]) if hrefs else None\n",
    "\n",
    "            alertas = item.get(\"CautionDescription\", [])\n",
    "            if not alertas:\n",
    "                todos_dados.append({\n",
    "                    \"Equipamento\": equipamento,\n",
    "                    \"DataHora\": None,\n",
    "                    \"Identifier\": None,\n",
    "                    \"Description\": None,\n",
    "                    \"Severity\": None,\n",
    "                    \"Active\": None,\n",
    "                    \"Arquivo\": arq\n",
    "                })\n",
    "            else:\n",
    "                for alerta in alertas:\n",
    "                    todos_dados.append({\n",
    "                        \"Equipamento\": equipamento,\n",
    "                        \"DataHora\": alerta.get(\"@datetime\"),\n",
    "                        \"Identifier\": alerta.get(\"Identifier\"),\n",
    "                        \"Description\": alerta.get(\"Description\"),\n",
    "                        \"Severity\": alerta.get(\"Severity\"),\n",
    "                        \"Active\": alerta.get(\"Active\"),\n",
    "                        \"Arquivo\": arq\n",
    "                    })\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Erro ao processar {arq}: {e}\")\n",
    "\n",
    "# Criação do DataFrame\n",
    "df = pd.DataFrame(todos_dados)\n",
    "\n",
    "# Extrai a data do nome do arquivo\n",
    "df[\"Data\"] = df[\"Arquivo\"].str.extract(r\"_(\\d{4}-\\d{2}-\\d{2})_\")\n",
    "\n",
    "# Organiza colunas\n",
    "df = df[[\"Equipamento\", \"Identifier\", \"Description\", \"Severity\", \"Active\", \"DataHora\", \"Data\"]]\n",
    "\n",
    "# Salva CSV\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"✅ CSV salvo em: {output_csv}\")\n",
    "\n",
    "# Atualiza o banco: DELETE + INSERT\n",
    "from sqlalchemy import text  # Import necessário\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(f'DELETE FROM \"Fazenda_WS\".\"{tabela_banco}\"'))\n",
    "    print(f\"🧹 Tabela {tabela_banco} limpa com DELETE\")\n",
    "\n",
    "df.to_sql(\n",
    "    tabela_banco,\n",
    "    engine,\n",
    "    schema=\"Fazenda_WS\",\n",
    "    if_exists=\"append\",\n",
    "    index=False,\n",
    "    method=\"multi\"  # performance otimizada\n",
    ")\n",
    "\n",
    "print(f\"✅ Dados atualizados no banco: Fazenda_WS.{tabela_banco} (Total: {len(df)} registros)\")\n",
    "\n",
    "\n",
    "# ### Parte 15 - Telemetry Average Load\n",
    "\n",
    "# In[34]:\n",
    "\n",
    "\n",
    "prefix = \"telemetry_average_load_\"\n",
    "output_csv = os.path.join(output_dir, \"telemetry_average_load.csv\")\n",
    "output_excel = os.path.join(output_dir, \"telemetry_average_load.xlsx\")\n",
    "tabela_banco = \"telemetry_average_load\"\n",
    "\n",
    "arquivos = listar_arquivos(prefix)\n",
    "todos_dados = []\n",
    "\n",
    "for arq in arquivos:\n",
    "    try:\n",
    "        dados = abrir_json(arq)\n",
    "        for item in dados:\n",
    "            links = item.get(\"Links\", [])\n",
    "            hrefs = [l[\"href\"] for l in links if l[\"rel\"] == \"self\"]\n",
    "            equipamento = extrair_equipamento(hrefs[0]) if hrefs else None\n",
    "\n",
    "            fatores = item.get(\"LoadFactor\", [])\n",
    "            if not fatores:\n",
    "                todos_dados.append({\n",
    "                    \"Equipamento\": equipamento,\n",
    "                    \"LoadFactor\": 0,\n",
    "                    \"DataHora\": None,\n",
    "                    \"Arquivo\": arq\n",
    "                })\n",
    "            else:\n",
    "                for f in fatores:\n",
    "                    todos_dados.append({\n",
    "                        \"Equipamento\": equipamento,\n",
    "                        \"LoadFactor\": f.get(\"LoadFactor\", 0),\n",
    "                        \"DataHora\": f.get(\"@datetime\"),\n",
    "                        \"Arquivo\": arq\n",
    "                    })\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Erro ao processar {arq}: {e}\")\n",
    "\n",
    "# DataFrame\n",
    "df = pd.DataFrame(todos_dados)\n",
    "\n",
    "# Extrai a data do nome do arquivo\n",
    "df[\"Data\"] = df[\"Arquivo\"].str.extract(r\"_(\\d{4}-\\d{2}-\\d{2})_\")\n",
    "\n",
    "# Organiza colunas\n",
    "df = df[[\"Equipamento\", \"LoadFactor\", \"DataHora\", \"Data\"]]\n",
    "\n",
    "# Salva CSV\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"✅ CSV salvo em: {output_csv}\")\n",
    "\n",
    "# Salva no banco\n",
    "df.to_sql(tabela_banco, engine, schema=\"Fazenda_WS\", if_exists=\"replace\", index=False)\n",
    "print(f\"✅ Tabela recriada no banco: Fazenda_WS.{tabela_banco}\")\n",
    "\n",
    "\n",
    "# ### Parte 16 - Operations By Vehicle\n",
    "\n",
    "# In[35]:\n",
    "\n",
    "\n",
    "# Parte 16 - Operations By Vehicle\n",
    "\n",
    "prefix = \"operations_by_vehicle_\"\n",
    "output_csv = os.path.join(output_dir, \"operations_by_vehicle.csv\")\n",
    "output_excel = os.path.join(output_dir, \"operations_by_vehicle.xlsx\")\n",
    "tabela_banco = \"operations_by_vehicle\"\n",
    "\n",
    "arquivos = listar_arquivos(prefix)\n",
    "todos_dados = []\n",
    "\n",
    "for arq in arquivos:\n",
    "    try:\n",
    "        dados = abrir_json(arq)\n",
    "        for item in dados:\n",
    "            company_id = item.get(\"companyId\")\n",
    "            operacoes = item.get(\"operationsByVehicles\", [])\n",
    "            if not operacoes:\n",
    "                todos_dados.append({\n",
    "                    \"companyId\": company_id,\n",
    "                    \"VehicleId\": None,\n",
    "                    \"OperationId\": None,\n",
    "                    \"OperationName\": None,\n",
    "                    \"Arquivo\": arq\n",
    "                })\n",
    "            else:\n",
    "                for op in operacoes:\n",
    "                    todos_dados.append({\n",
    "                        \"companyId\": company_id,\n",
    "                        \"VehicleId\": op.get(\"vehicleId\"),\n",
    "                        \"OperationId\": op.get(\"operationId\"),\n",
    "                        \"OperationName\": op.get(\"operationName\"),\n",
    "                        \"Arquivo\": arq\n",
    "                    })\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Erro ao processar {arq}: {e}\")\n",
    "\n",
    "# DataFrame\n",
    "df = pd.DataFrame(todos_dados)\n",
    "\n",
    "# Extrai a data do nome do arquivo\n",
    "df[\"Data\"] = df[\"Arquivo\"].str.extract(r\"_(\\d{4}-\\d{2}-\\d{2})_\")\n",
    "\n",
    "# Organiza colunas\n",
    "df = df[[\"companyId\", \"VehicleId\", \"OperationId\", \"OperationName\", \"Data\"]]\n",
    "\n",
    "# Salva CSV\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"✅ CSV salvo em: {output_csv}\")\n",
    "\n",
    "# Salva no banco\n",
    "df.to_sql(tabela_banco, engine, schema=\"Fazenda_WS\", if_exists=\"replace\", index=False)\n",
    "print(f\"✅ Tabela recriada no banco: Fazenda_WS.{tabela_banco}\")\n",
    "\n",
    "\n",
    "# ### Dimensões\n",
    "\n",
    "# In[36]:\n",
    "\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import sqlalchemy \n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "\n",
    "# Configurações de diretório e HDFS\n",
    "output_dir = r\"C:\\Users\\caval\\Projetos\\COLIN_WS\\csv_corrigidos\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "namenode = \"172.21.0.142\"\n",
    "port = 9870\n",
    "user = \"hdoop\"\n",
    "hdfs_path = \"/bronze/agrin/suprema/dim/equipment.json\"\n",
    "\n",
    "# Conexão com banco\n",
    "password = quote_plus(\"@Winover2024\")\n",
    "engine = create_engine(\n",
    "    f\"postgresql+psycopg2://wesley.carnauba:{password}@172.21.0.119/warin\",\n",
    "    connect_args={'client_encoding': 'utf8'}\n",
    ")\n",
    "\n",
    "# Função para baixar JSON diretamente do HDFS\n",
    "def ler_json_hdfs(caminho_hdfs):\n",
    "    url = f\"http://{namenode}:{port}/webhdfs/v1{caminho_hdfs}?op=OPEN&user.name={user}\"\n",
    "    r1 = requests.get(url, allow_redirects=False)\n",
    "    redirect = r1.headers.get(\"Location\")\n",
    "    if not redirect:\n",
    "        raise Exception(\"❌ Redirecionamento HDFS falhou.\")\n",
    "    redirect = redirect.replace(\"trisk05\", namenode).replace(\"trisk06\", namenode).replace(\"trisk10\", namenode)\n",
    "    r2 = requests.get(redirect)\n",
    "    r2.raise_for_status()\n",
    "    return r2.json()\n",
    "\n",
    "# Lê do HDFS\n",
    "dados = ler_json_hdfs(hdfs_path)\n",
    "\n",
    "from sqlalchemy import text\n",
    "\n",
    "from sqlalchemy import text\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# dim_equipment\n",
    "\n",
    "equipamentos = dados[\"equipment\"]\n",
    "df = pd.json_normalize(equipamentos)\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].apply(lambda x: isinstance(x, (dict, list))).any():\n",
    "        df[col] = df[col].apply(lambda x: json.dumps(x, ensure_ascii=False) if isinstance(x, (dict, list)) else x)\n",
    "\n",
    "csv_path = os.path.join(output_dir, \"dim_equipment.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"✅ CSV salvo em: {csv_path}\")\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text('DELETE FROM \"Fazenda_WS\".dim_equipment'))\n",
    "    df.to_sql(\"dim_equipment\", conn, schema=\"Fazenda_WS\", if_exists=\"append\", index=False)\n",
    "print(\"✅ Tabela dim_equipment atualizada no schema Fazenda_WS\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# dim_farm_setup_company_details\n",
    "\n",
    "hdfs_path = \"/bronze/agrin/suprema/dim/farm_setup_company_details.json\"\n",
    "dados = ler_json_hdfs(hdfs_path)\n",
    "df = pd.json_normalize(dados[\"companies\"])\n",
    "\n",
    "csv_path = os.path.join(output_dir, \"dim_farm_setup_company_details.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"✅ CSV salvo em: {csv_path}\")\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text('DELETE FROM \"Fazenda_WS\".dim_farm_setup_company_details'))\n",
    "    df.to_sql(\"dim_farm_setup_company_details\", conn, schema=\"Fazenda_WS\", if_exists=\"append\", index=False)\n",
    "print(\"✅ Tabela dim_farm_setup_company_details atualizada no schema Fazenda_WS\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# dim_farm_setup_fields\n",
    "\n",
    "hdfs_path = \"/bronze/agrin/suprema/dim/farm_setup_fields.json\"\n",
    "dados = ler_json_hdfs(hdfs_path)\n",
    "\n",
    "company_id = dados.get(\"companyId\")\n",
    "grower_id = dados.get(\"growerId\")\n",
    "farm_id = dados.get(\"farmId\")\n",
    "\n",
    "fields = dados.get(\"fields\", [])\n",
    "df = pd.json_normalize(fields)\n",
    "df[\"companyId\"] = company_id\n",
    "df[\"growerId\"] = grower_id\n",
    "df[\"farmId\"] = farm_id\n",
    "\n",
    "csv_path = os.path.join(output_dir, \"dim_farm_setup_fields.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"✅ CSV salvo em: {csv_path}\")\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text('DELETE FROM \"Fazenda_WS\".dim_farm_setup_fields'))\n",
    "    df.to_sql(\"dim_farm_setup_fields\", conn, schema=\"Fazenda_WS\", if_exists=\"append\", index=False)\n",
    "print(\"✅ Tabela dim_farm_setup_fields atualizada no schema Fazenda_WS\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# dim_farm_setup_growers\n",
    "\n",
    "hdfs_path = \"/bronze/agrin/suprema/dim/farm_setup_growers.json\"\n",
    "dados = ler_json_hdfs(hdfs_path)\n",
    "\n",
    "company_id = dados.get(\"companyId\")\n",
    "growers = dados.get(\"growers\", [])\n",
    "df = pd.json_normalize(growers)\n",
    "df[\"companyId\"] = company_id\n",
    "\n",
    "csv_path = os.path.join(output_dir, \"dim_farm_setup_growers.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"✅ CSV salvo em: {csv_path}\")\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text('DELETE FROM \"Fazenda_WS\".dim_farm_setup_growers'))\n",
    "    df.to_sql(\"dim_farm_setup_growers\", conn, schema=\"Fazenda_WS\", if_exists=\"append\", index=False)\n",
    "print(\"✅ Tabela dim_farm_setup_growers atualizada no schema Fazenda_WS\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# dim_farm_setup_user_profile\n",
    "\n",
    "hdfs_path = \"/bronze/agrin/suprema/dim/farm_setup_user_profile.json\"\n",
    "dados = ler_json_hdfs(hdfs_path)\n",
    "\n",
    "df = pd.DataFrame([dados])\n",
    "\n",
    "csv_path = os.path.join(output_dir, \"dim_farm_setup_user_profile.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"✅ CSV salvo em: {csv_path}\")\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text('DELETE FROM \"Fazenda_WS\".dim_farm_setup_user_profile'))\n",
    "    df.to_sql(\"dim_farm_setup_user_profile\", conn, schema=\"Fazenda_WS\", if_exists=\"append\", index=False)\n",
    "print(\"✅ Tabela dim_farm_setup_user_profile atualizada no schema Fazenda_WS\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# dim_files\n",
    "\n",
    "hdfs_path = \"/bronze/agrin/suprema/dim/files.json\"\n",
    "nome_dim = \"dim_files\"\n",
    "\n",
    "dados = ler_json_hdfs(hdfs_path)\n",
    "df = pd.json_normalize(dados[\"files\"])\n",
    "\n",
    "csv_path = os.path.join(output_dir, f\"{nome_dim}.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"✅ CSV salvo em: {csv_path}\")\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(f'DELETE FROM \"Fazenda_WS\".{nome_dim}'))\n",
    "    df.to_sql(nome_dim, conn, schema=\"Fazenda_WS\", if_exists=\"append\", index=False)\n",
    "print(f\"✅ Tabela {nome_dim} atualizada no schema Fazenda_WS\")\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# dim_rx_direct_to_vehicle_activity_types\n",
    "\n",
    "hdfs_path = \"/bronze/agrin/suprema/dim/rx_direct_to_vehicle_activity_types.json\"\n",
    "nome_dim = \"dim_rx_direct_to_vehicle_activity_types\"\n",
    "\n",
    "dados = ler_json_hdfs(hdfs_path)\n",
    "df = pd.json_normalize(dados[\"activityTypes\"])\n",
    "\n",
    "csv_path = os.path.join(output_dir, f\"{nome_dim}.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"✅ CSV salvo em: {csv_path}\")\n",
    "\n",
    "df.to_sql(nome_dim, engine, schema=\"Fazenda_WS\", if_exists=\"replace\", index=False)\n",
    "print(f\"✅ Tabela {nome_dim} criada/recriada no schema Fazenda_WS\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# dim_rx_direct_to_vehicle_chemical_categories\n",
    "\n",
    "hdfs_path = \"/bronze/agrin/suprema/dim/rx_direct_to_vehicle_chemical_categories.json\"\n",
    "nome_dim = \"dim_rx_direct_to_vehicle_chemical_categories\"\n",
    "\n",
    "dados = ler_json_hdfs(hdfs_path)\n",
    "df = pd.json_normalize(dados[\"categories\"])\n",
    "\n",
    "csv_path = os.path.join(output_dir, f\"{nome_dim}.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"✅ CSV salvo em: {csv_path}\")\n",
    "\n",
    "df.to_sql(nome_dim, engine, schema=\"Fazenda_WS\", if_exists=\"replace\", index=False)\n",
    "print(f\"✅ Tabela {nome_dim} criada/recriada no schema Fazenda_WS\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# dim_rx_direct_to_vehicle_chemicals\n",
    "\n",
    "hdfs_path = \"/bronze/agrin/suprema/dim/rx_direct_to_vehicle_chemicals.json\"\n",
    "nome_dim = \"dim_rx_direct_to_vehicle_chemicals\"\n",
    "\n",
    "dados = ler_json_hdfs(hdfs_path)\n",
    "df = pd.json_normalize(dados[\"chemicals\"])\n",
    "\n",
    "csv_path = os.path.join(output_dir, f\"{nome_dim}.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"✅ CSV salvo em: {csv_path}\")\n",
    "\n",
    "df.to_sql(nome_dim, engine, schema=\"Fazenda_WS\", if_exists=\"replace\", index=False)\n",
    "print(f\"✅ Tabela {nome_dim} criada/recriada no schema Fazenda_WS\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# dim_rx_direct_to_vehicle_crops\n",
    "\n",
    "hdfs_path = \"/bronze/agrin/suprema/dim/rx_direct_to_vehicle_crops.json\"\n",
    "nome_dim = \"dim_rx_direct_to_vehicle_crops\"\n",
    "\n",
    "dados = ler_json_hdfs(hdfs_path)\n",
    "df = pd.json_normalize(dados[\"crops\"])\n",
    "\n",
    "csv_path = os.path.join(output_dir, f\"{nome_dim}.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"✅ CSV salvo em: {csv_path}\")\n",
    "\n",
    "df.to_sql(nome_dim, engine, schema=\"Fazenda_WS\", if_exists=\"replace\", index=False)\n",
    "print(f\"✅ Tabela {nome_dim} criada/recriada no schema Fazenda_WS\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# dim_rx_direct_to_vehicle_element_types\n",
    "\n",
    "hdfs_path = \"/bronze/agrin/suprema/dim/rx_direct_to_vehicle_element_types.json\"\n",
    "nome_dim = \"dim_rx_direct_to_vehicle_element_types\"\n",
    "\n",
    "dados = ler_json_hdfs(hdfs_path)\n",
    "df = pd.json_normalize(dados[\"elementTypes\"])\n",
    "\n",
    "csv_path = os.path.join(output_dir, f\"{nome_dim}.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"✅ CSV salvo em: {csv_path}\")\n",
    "\n",
    "df.to_sql(nome_dim, engine, schema=\"Fazenda_WS\", if_exists=\"replace\", index=False)\n",
    "print(f\"✅ Tabela {nome_dim} criada/recriada no schema Fazenda_WS\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# dim_rx_direct_to_vehicle_genetic_types\n",
    "\n",
    "hdfs_path = \"/bronze/agrin/suprema/dim/rx_direct_to_vehicle_genetic_types.json\"\n",
    "nome_dim = \"dim_rx_direct_to_vehicle_genetic_types\"\n",
    "\n",
    "dados = ler_json_hdfs(hdfs_path)\n",
    "df = pd.json_normalize(dados[\"geneticTypes\"])\n",
    "\n",
    "csv_path = os.path.join(output_dir, f\"{nome_dim}.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"✅ CSV salvo em: {csv_path}\")\n",
    "\n",
    "df.to_sql(nome_dim, engine, schema=\"Fazenda_WS\", if_exists=\"replace\", index=False)\n",
    "print(f\"✅ Tabela {nome_dim} criada/recriada no schema Fazenda_WS\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# dim_rx_direct_to_vehicle_product_forms\n",
    "\n",
    "hdfs_path = \"/bronze/agrin/suprema/dim/rx_direct_to_vehicle_product_forms.json\"\n",
    "nome_dim = \"dim_rx_direct_to_vehicle_product_forms\"\n",
    "\n",
    "dados = ler_json_hdfs(hdfs_path)\n",
    "df = pd.json_normalize(dados[\"productForms\"])\n",
    "\n",
    "csv_path = os.path.join(output_dir, f\"{nome_dim}.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"✅ CSV salvo em: {csv_path}\")\n",
    "\n",
    "df.to_sql(nome_dim, engine, schema=\"Fazenda_WS\", if_exists=\"replace\", index=False)\n",
    "print(f\"✅ Tabela {nome_dim} criada/recriada no schema Fazenda_WS\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# dim_rx_direct_to_vehicle_seasons\n",
    "\n",
    "hdfs_path = \"/bronze/agrin/suprema/dim/rx_direct_to_vehicle_seasons.json\"\n",
    "nome_dim = \"dim_rx_direct_to_vehicle_seasons\"\n",
    "\n",
    "dados = ler_json_hdfs(hdfs_path)\n",
    "df = pd.json_normalize(dados[\"seasons\"])\n",
    "\n",
    "csv_path = os.path.join(output_dir, f\"{nome_dim}.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"✅ CSV salvo em: {csv_path}\")\n",
    "\n",
    "df.to_sql(nome_dim, engine, schema=\"Fazenda_WS\", if_exists=\"replace\", index=False)\n",
    "print(f\"✅ Tabela {nome_dim} criada/recriada no schema Fazenda_WS\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# dim_rx_direct_to_vehicle_seed_brands\n",
    "\n",
    "hdfs_path = \"/bronze/agrin/suprema/dim/rx_direct_to_vehicle_seed_brands.json\"\n",
    "nome_dim = \"dim_rx_direct_to_vehicle_seed_brands\"\n",
    "\n",
    "dados = ler_json_hdfs(hdfs_path)\n",
    "df = pd.json_normalize(dados[\"brands\"])\n",
    "\n",
    "csv_path = os.path.join(output_dir, f\"{nome_dim}.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"✅ CSV salvo em: {csv_path}\")\n",
    "\n",
    "df.to_sql(nome_dim, engine, schema=\"Fazenda_WS\", if_exists=\"replace\", index=False)\n",
    "print(f\"✅ Tabela {nome_dim} criada/recriada no schema Fazenda_WS\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# dim_rx_direct_to_vehicle_seed_manufacturers\n",
    "\n",
    "hdfs_path = \"/bronze/agrin/suprema/dim/rx_direct_to_vehicle_seed_manufacturers.json\"\n",
    "nome_dim = \"dim_rx_direct_to_vehicle_seed_manufacturers\"\n",
    "\n",
    "dados = ler_json_hdfs(hdfs_path)\n",
    "df = pd.json_normalize(dados[\"manufacturers\"])\n",
    "\n",
    "csv_path = os.path.join(output_dir, f\"{nome_dim}.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"✅ CSV salvo em: {csv_path}\")\n",
    "\n",
    "df.to_sql(nome_dim, engine, schema=\"Fazenda_WS\", if_exists=\"replace\", index=False)\n",
    "print(f\"✅ Tabela {nome_dim} criada/recriada no schema Fazenda_WS\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# dim_rx_direct_to_vehicle_seeds\n",
    "\n",
    "hdfs_path = \"/bronze/agrin/suprema/dim/rx_direct_to_vehicle_seeds.json\"\n",
    "nome_dim = \"dim_rx_direct_to_vehicle_seeds\"\n",
    "\n",
    "dados = ler_json_hdfs(hdfs_path)\n",
    "df = pd.json_normalize(dados[\"seeds\"])\n",
    "\n",
    "csv_path = os.path.join(output_dir, f\"{nome_dim}.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"✅ CSV salvo em: {csv_path}\")\n",
    "\n",
    "df.to_sql(nome_dim, engine, schema=\"Fazenda_WS\", if_exists=\"replace\", index=False)\n",
    "print(f\"✅ Tabela {nome_dim} criada/recriada no schema Fazenda_WS\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# dim_rx_direct_to_vehicle_units_of_measure\n",
    "\n",
    "hdfs_path = \"/bronze/agrin/suprema/dim/rx_direct_to_vehicle_units_of_measure.json\"\n",
    "nome_dim = \"dim_rx_direct_to_vehicle_units_of_measure\"\n",
    "\n",
    "dados = ler_json_hdfs(hdfs_path)\n",
    "df = pd.json_normalize(dados[\"unitsOfMeasure\"])\n",
    "\n",
    "csv_path = os.path.join(output_dir, f\"{nome_dim}.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"✅ CSV salvo em: {csv_path}\")\n",
    "\n",
    "df.to_sql(nome_dim, engine, schema=\"Fazenda_WS\", if_exists=\"replace\", index=False)\n",
    "print(f\"✅ Tabela {nome_dim} criada/recriada no schema Fazenda_WS\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# dim_rx_direct_to_vehicle_units_of_measure_types\n",
    "\n",
    "hdfs_path = \"/bronze/agrin/suprema/dim/rx_direct_to_vehicle_units_of_measure_types.json\"\n",
    "nome_dim = \"dim_rx_direct_to_vehicle_units_of_measure_types\"\n",
    "\n",
    "dados = ler_json_hdfs(hdfs_path)\n",
    "df = pd.json_normalize(dados[\"unitOfMeasureTypes\"])\n",
    "\n",
    "csv_path = os.path.join(output_dir, f\"{nome_dim}.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"✅ CSV salvo em: {csv_path}\")\n",
    "\n",
    "df.to_sql(nome_dim, engine, schema=\"Fazenda_WS\", if_exists=\"replace\", index=False)\n",
    "print(f\"✅ Tabela {nome_dim} criada/recriada no schema Fazenda_WS\")\n",
    "\n",
    "\n",
    "\n",
    "# In[37]:\n",
    "\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from urllib.parse import quote_plus\n",
    "from datetime import datetime\n",
    "\n",
    "# Configurações\n",
    "API_KEY = 'AIzaSyC0C0TPNokotbZCDwV-0c5DZ8gS4lf49aY'\n",
    "URL_FORECAST = 'https://weather.googleapis.com/v1/forecast/days:lookup'\n",
    "latitude = -12.5420\n",
    "longitude = -55.7211\n",
    "\n",
    "# Conexão com PostgreSQL\n",
    "password = quote_plus(\"@Winover2024\")\n",
    "engine = create_engine(\n",
    "    f\"postgresql+psycopg2://wesley.carnauba:{password}@172.21.0.119/warin\",\n",
    "    connect_args={'client_encoding': 'utf8'}\n",
    ")\n",
    "\n",
    "# Dicionários de tradução\n",
    "TRADUCOES = {\n",
    "    'weather_conditions': {\n",
    "        'Mostly cloudy': 'predominantemente nublado',\n",
    "        'Clear': 'céu limpo',\n",
    "        'Sunny': 'ensolarado',\n",
    "        'Partly sunny': 'parcialmente ensolarado',\n",
    "        'Mostly sunny': 'predominantemente ensolarado',\n",
    "        'Clear with periodic clouds': 'céu limpo com nuvens periódicas'\n",
    "    },\n",
    "    'precipitation_types': {\n",
    "        'RAIN': 'chuva',\n",
    "        'SNOW': 'neve',\n",
    "        'ICE': 'gelo',\n",
    "        'NONE': 'nenhuma'\n",
    "    }\n",
    "}\n",
    "\n",
    "def safe_get_value(obj, *keys):\n",
    "    \"\"\"Percorre os níveis do dicionário e retorna o valor se existir\"\"\"\n",
    "    for key in keys:\n",
    "        if not isinstance(obj, dict):\n",
    "            return None\n",
    "        obj = obj.get(key)\n",
    "    return obj\n",
    "\n",
    "def get_weather_forecast():\n",
    "    \"\"\"Busca a previsão do tempo da API Google\"\"\"\n",
    "    params = {\n",
    "        'location.latitude': latitude,\n",
    "        'location.longitude': longitude,\n",
    "        'key': API_KEY\n",
    "    }\n",
    "    response = requests.get(URL_FORECAST, params=params)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "def translate_values(data_dict):\n",
    "    \"\"\"Traduz os valores para português\"\"\"\n",
    "    if data_dict.get('condicao_climatica') in TRADUCOES['weather_conditions']:\n",
    "        data_dict['condicao_climatica'] = TRADUCOES['weather_conditions'][data_dict['condicao_climatica']]\n",
    "    if data_dict.get('tipo_precipitacao') in TRADUCOES['precipitation_types']:\n",
    "        data_dict['tipo_precipitacao'] = TRADUCOES['precipitation_types'][data_dict['tipo_precipitacao']]\n",
    "    return data_dict\n",
    "\n",
    "def create_forecast_df(forecast_data):\n",
    "    \"\"\"Cria DataFrame com os dados da previsão\"\"\"\n",
    "    table_data = []\n",
    "\n",
    "    # Temperatura atual (se existir)\n",
    "    current_conditions = forecast_data.get('currentConditions', {})\n",
    "    current_temp = safe_get_value(current_conditions, 'temperature', 'value')\n",
    "    current_time = safe_get_value(current_conditions, 'asOf')\n",
    "\n",
    "    if current_temp is not None and current_time is not None:\n",
    "        table_data.append({\n",
    "            'data_previsao': current_time[:10],\n",
    "            'periodo': 'atual',\n",
    "            'condicao_climatica': safe_get_value(current_conditions, 'weatherCondition', 'description', 'text'),\n",
    "            'probabilidade_chuva': None,\n",
    "            'tipo_precipitacao': None,\n",
    "            'cobertura_nuvens': safe_get_value(current_conditions, 'cloudCover'),\n",
    "            'umidade_relativa': safe_get_value(current_conditions, 'relativeHumidity'),\n",
    "            'temperatura_maxima': current_temp,\n",
    "            'temperatura_minima': None,\n",
    "            'velocidade_vento': safe_get_value(current_conditions, 'wind', 'speed', 'value')\n",
    "        })\n",
    "\n",
    "    for day in forecast_data.get('forecastDays', []):\n",
    "        daytime = day.get('daytimeForecast', {})\n",
    "        nighttime = day.get('nighttimeForecast', {})\n",
    "        date = safe_get_value(daytime, 'interval', 'startTime')\n",
    "        date = date[:10] if date else None\n",
    "\n",
    "        if not date:\n",
    "            continue\n",
    "\n",
    "        max_temp = day.get('maxTemperature', {}).get('degrees')\n",
    "        min_temp = day.get('minTemperature', {}).get('degrees')\n",
    "\n",
    "        day_data = {\n",
    "            'data_previsao': date,\n",
    "            'periodo': 'diurno',\n",
    "            'condicao_climatica': safe_get_value(daytime, 'weatherCondition', 'description', 'text'),\n",
    "            'probabilidade_chuva': safe_get_value(daytime, 'precipitation', 'probability', 'percent'),\n",
    "            'tipo_precipitacao': safe_get_value(daytime, 'precipitation', 'probability', 'type'),\n",
    "            'cobertura_nuvens': daytime.get('cloudCover'),\n",
    "            'umidade_relativa': daytime.get('relativeHumidity'),\n",
    "            'temperatura_maxima': max_temp,\n",
    "            'temperatura_minima': min_temp,  # Aplicado aqui também\n",
    "            'velocidade_vento': safe_get_value(daytime, 'wind', 'speed', 'value')\n",
    "        }\n",
    "\n",
    "        night_data = {\n",
    "            'data_previsao': date,\n",
    "            'periodo': 'noturno',\n",
    "            'condicao_climatica': safe_get_value(nighttime, 'weatherCondition', 'description', 'text'),\n",
    "            'probabilidade_chuva': safe_get_value(nighttime, 'precipitation', 'probability', 'percent'),\n",
    "            'tipo_precipitacao': safe_get_value(nighttime, 'precipitation', 'probability', 'type'),\n",
    "            'cobertura_nuvens': nighttime.get('cloudCover'),\n",
    "            'umidade_relativa': nighttime.get('relativeHumidity'),\n",
    "            'temperatura_maxima': max_temp,  # Também aqui\n",
    "            'temperatura_minima': min_temp,\n",
    "            'velocidade_vento': safe_get_value(nighttime, 'wind', 'speed', 'value')\n",
    "        }\n",
    "\n",
    "        table_data.extend([translate_values(day_data), translate_values(night_data)])\n",
    "\n",
    "    return pd.DataFrame(table_data).dropna(subset=['data_previsao'])\n",
    "\n",
    "def get_existing_dates():\n",
    "    \"\"\"Obtém as datas já existentes na tabela\"\"\"\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(\"\"\"\n",
    "            SELECT DISTINCT data_previsao \n",
    "            FROM \"Fazenda_WS\".forecast\n",
    "            WHERE data_previsao >= CURRENT_DATE\n",
    "        \"\"\"))\n",
    "        return {row[0] for row in result}\n",
    "\n",
    "def update_forecast_data():\n",
    "    \"\"\"Atualiza os dados de previsão na tabela\"\"\"\n",
    "    try:\n",
    "        print(\"📡 Obtendo dados atualizados da API...\")\n",
    "        api_data = get_weather_forecast()\n",
    "        forecast_df = create_forecast_df(api_data)\n",
    "\n",
    "        if forecast_df.empty:\n",
    "            print(\"⚠️ Nenhum dado válido recebido da API.\")\n",
    "            return False\n",
    "\n",
    "        forecast_df['data_previsao'] = pd.to_datetime(forecast_df['data_previsao'])\n",
    "        forecast_df = forecast_df[forecast_df['data_previsao'] >= pd.to_datetime(datetime.now().date())]\n",
    "\n",
    "        if forecast_df.empty:\n",
    "            print(\"⚠️ Nenhum dado futuro ou atual disponível.\")\n",
    "            return False\n",
    "\n",
    "        existing_dates = get_existing_dates()\n",
    "        new_dates = set(forecast_df['data_previsao'].dt.date) - existing_dates\n",
    "\n",
    "        with engine.begin() as conn:\n",
    "            # Atualiza registros existentes\n",
    "            for _, row in forecast_df[forecast_df['data_previsao'].dt.date.isin(existing_dates)].iterrows():\n",
    "                conn.execute(text(\"\"\"\n",
    "                    UPDATE \"Fazenda_WS\".forecast \n",
    "                    SET condicao_climatica = :condicao,\n",
    "                        probabilidade_chuva = :prob_chuva,\n",
    "                        tipo_precipitacao = :tipo_precip,\n",
    "                        cobertura_nuvens = :nuvens,\n",
    "                        umidade_relativa = :umidade,\n",
    "                        temperatura_maxima = COALESCE(:temp_max, temperatura_maxima),\n",
    "                        temperatura_minima = COALESCE(:temp_min, temperatura_minima),\n",
    "                        velocidade_vento = :vento\n",
    "                    WHERE data_previsao = :data AND periodo = :periodo\n",
    "                \"\"\"), {\n",
    "                    'data': row['data_previsao'],\n",
    "                    'periodo': row['periodo'],\n",
    "                    'condicao': row['condicao_climatica'],\n",
    "                    'prob_chuva': row['probabilidade_chuva'],\n",
    "                    'tipo_precip': row['tipo_precipitacao'],\n",
    "                    'nuvens': row['cobertura_nuvens'],\n",
    "                    'umidade': row['umidade_relativa'],\n",
    "                    'temp_max': row['temperatura_maxima'],\n",
    "                    'temp_min': row['temperatura_minima'],\n",
    "                    'vento': row['velocidade_vento']\n",
    "                })\n",
    "\n",
    "            # Insere novos registros\n",
    "            new_data = forecast_df[forecast_df['data_previsao'].dt.date.isin(new_dates)]\n",
    "            if not new_data.empty:\n",
    "                new_data.to_sql(\n",
    "                    name='forecast',\n",
    "                    con=conn,\n",
    "                    schema='Fazenda_WS',\n",
    "                    if_exists='append',\n",
    "                    index=False\n",
    "                )\n",
    "\n",
    "        print(f\"✅ Atualização concluída! {len(existing_dates)} datas atualizadas, {len(new_dates)} novas datas adicionadas.\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erro durante a atualização: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    update_forecast_data()\n",
    "\n",
    "\n",
    "# In[38]:\n",
    "\n",
    "\n",
    "# Leitura do CSV ignorando a segunda linha\n",
    "df = pd.read_csv(r\"C:\\Users\\caval\\Projetos\\COLIN_WS\\files\\report_weather.csv\", sep=\";\", skiprows=[1])\n",
    "\n",
    "# Normalização dos nomes das colunas\n",
    "def normalize_column(name):\n",
    "    name = name.strip().lower()\n",
    "    name = unicodedata.normalize('NFKD', name).encode('ASCII', 'ignore').decode('ASCII')  # Remove acentos\n",
    "    name = re.sub(r'[^\\w\\s]', '', name)  # Remove caracteres especiais\n",
    "    name = re.sub(r'\\s+', '_', name)     # Substitui espaços por underscores\n",
    "    return name\n",
    "\n",
    "df.columns = [normalize_column(col) for col in df.columns]\n",
    "\n",
    "# Corrigir separadores decimais (vírgula para ponto) em colunas de texto\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == object:\n",
    "        df[col] = df[col].str.replace(',', '.', regex=False)\n",
    "\n",
    "# Tentar converter colunas para numérico\n",
    "for col in df.columns:\n",
    "    try:\n",
    "        df[col] = pd.to_numeric(df[col])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Conexão com PostgreSQL\n",
    "password = quote_plus(\"@Winover2024\")\n",
    "engine = create_engine(\n",
    "    f\"postgresql+psycopg2://wesley.carnauba:{password}@172.21.0.119/warin\",\n",
    "    connect_args={'client_encoding': 'utf8'}\n",
    ")\n",
    "\n",
    "# Subir os dados para o schema \"Fazenda_WS\"\n",
    "df.to_sql(\"clima_fazenda_ws\", engine, schema=\"Fazenda_WS\", if_exists=\"replace\", index=False)\n",
    "\n",
    "print(\"Dados importados com sucesso para o schema Fazenda_WS.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
